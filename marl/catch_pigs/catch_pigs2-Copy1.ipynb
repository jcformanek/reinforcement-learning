{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stunning-buyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "\n",
    "from env_CatchPigs import EnvCatchPigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cardiovascular-comedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, size, obs_dims):\n",
    "        self.mem_size = size\n",
    "        self.obs_mem = torch.zeros((self.mem_size, *obs_dims))\n",
    "        self.act_mem = torch.zeros(self.mem_size, dtype=torch.int64)\n",
    "        self.rew_mem = torch.zeros(self.mem_size, dtype=torch.float32)\n",
    "        self.next_obs_mem = torch.zeros((self.mem_size, *obs_dims))\n",
    "        self.done_mem = torch.zeros(self.mem_size, dtype=torch.bool)\n",
    "        self.cntr = 0\n",
    "\n",
    "    def push(self, obs, act, rew, next_obs, done):\n",
    "        \"\"\"\n",
    "        obs :: torch tensor shape==(channels, height, width)\n",
    "        act :: int\n",
    "        rew :: int\n",
    "        obs_ :: torch tensor shape==(channels, height, width)\n",
    "        done :: bool\n",
    "        \"\"\"\n",
    "\n",
    "        idx = self.cntr % self.mem_size\n",
    "        self.obs_mem[idx] = obs\n",
    "        self.act_mem[idx] = act\n",
    "        self.rew_mem[idx] = rew\n",
    "        self.next_obs_mem[idx] = next_obs\n",
    "        self.done_mem[idx] = done \n",
    "        self.cntr += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        max_idx = min(self.mem_size, self.cntr)\n",
    "        idxs = np.random.choice(max_idx, batch_size, replace=False)\n",
    "        obs_batch = self.obs_mem[idxs]\n",
    "        act_batch = self.act_mem[idxs]\n",
    "        rew_batch = self.rew_mem[idxs]\n",
    "        next_obs_batch = self.next_obs_mem[idxs]\n",
    "        done_batch = self.done_mem[idxs]\n",
    "\n",
    "        return obs_batch, act_batch, rew_batch, next_obs_batch, done_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "naughty-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, obs_dims, num_acts, lr=1e-3):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(obs_dims[0], 32, 4, stride=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 2, stride=1)\n",
    "        \n",
    "        linear_input_dims = self._calc_conv_output_dims(obs_dims)\n",
    "        \n",
    "        self.linear1 = nn.Linear(linear_input_dims, 512)\n",
    "        self.linear2 = nn.Linear(512, num_acts)\n",
    "        \n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "        \n",
    "    def _calc_conv_output_dims(self, input_dims):\n",
    "        tmp = torch.zeros((1, *input_dims))\n",
    "        tmp = self.conv1(tmp)\n",
    "        tmp = self.conv2(tmp)\n",
    "        tmp = self.conv3(tmp)\n",
    "        return int(np.prod(tmp.size()))\n",
    "    \n",
    "    \n",
    "    def forward(self, obs):\n",
    "        h = F.relu(self.conv1(obs))\n",
    "        h = F.relu(self.conv2(h))\n",
    "        h = F.relu(self.conv3(h))\n",
    "        # flatten conv layer output\n",
    "        h = h.view(h.size()[0], -1)\n",
    "        # conv_state shape is BS x (n_filters * H * W)\n",
    "        h = F.relu(self.linear1(h))\n",
    "        acts = self.linear2(h)\n",
    "\n",
    "        return acts\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "touched-montana",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    def __init__(self, obs_dims, num_acts, gamma=0.99, epsilon=1, lr=0.01,\n",
    "                 mem_size=10000, batch_size=32, eps_min=0.01, eps_dec=3e-4,\n",
    "                 replace=100, chkpt_dir='tmp/dqn'):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.lr = lr\n",
    "        self.num_acts = num_acts\n",
    "        self.obs_dims = obs_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.eps_min = eps_min\n",
    "        self.eps_dec = eps_dec\n",
    "        self.replace_target_cnt = replace\n",
    "        self.chkpt_dir = chkpt_dir\n",
    "        self.act_space = [i for i in range(num_acts)]\n",
    "        self.learn_cnt = 0\n",
    "\n",
    "        self.memory = ReplayBuffer(mem_size, obs_dims)\n",
    "        self.q_eval = DeepQNetwork(self.obs_dims, self.num_acts, self.lr)\n",
    "        self.q_next = DeepQNetwork(self.obs_dims, self.num_acts, self.lr)\n",
    "        \n",
    "        \n",
    "    def choose_act(self, obs):\n",
    "        \"\"\"\n",
    "        obs :: torch tensor shape==(3, 96, 96)\n",
    "        \"\"\"\n",
    "        if np.random.random() > self.epsilon:\n",
    "            acts = self.q_eval.forward(obs.unsqueeze(0))\n",
    "            act = torch.argmax(acts).item()\n",
    "        else:\n",
    "            act = np.random.choice(self.act_space)\n",
    "\n",
    "        return int(act)\n",
    "    \n",
    "    \n",
    "    def store_transition(self, obs, act, rew, next_obs, done):\n",
    "        self.memory.push(obs, act, rew, next_obs, done)\n",
    "        \n",
    "        \n",
    "    def sample_memory(self):\n",
    "        obs, act, rew, next_obs, done = self.memory.sample(self.batch_size)\n",
    "        return obs, act, rew, next_obs, done\n",
    "    \n",
    "    \n",
    "    def replace_target_network(self):\n",
    "        if self.learn_cnt % self.replace_target_cnt == 0:\n",
    "            self.q_next.load_state_dict(self.q_eval.state_dict())\n",
    "\n",
    "            \n",
    "    def decrement_epsilon(self):\n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
    "        \n",
    "    \n",
    "    def learn(self):\n",
    "        if self.memory.cntr < self.batch_size:\n",
    "            return\n",
    "\n",
    "        self.q_eval.optimizer.zero_grad()\n",
    "\n",
    "        self.replace_target_network()\n",
    "\n",
    "        obs, act, rew, next_obs, done = self.sample_memory()\n",
    "        indxs = np.arange(self.batch_size)\n",
    "\n",
    "        q_pred = self.q_eval.forward(obs)[indxs, act]\n",
    "        q_next = self.q_next.forward(next_obs).max(dim=1)[0]\n",
    "\n",
    "        q_next[done] = 0.0\n",
    "        q_target = rew + self.gamma * q_next\n",
    "\n",
    "        loss = self.q_eval.loss(q_target, q_pred)\n",
    "        loss.backward()\n",
    "        self.q_eval.optimizer.step()\n",
    "        self.learn_cnt += 1\n",
    "\n",
    "        self.decrement_epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "comfortable-internship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(max_iter, env, agent1, agent2):\n",
    "    \n",
    "    obs_list = env.get_obs()\n",
    "    obs1 = torch.tensor(obs_list[0], dtype=torch.float).permute(2, 0, 1)\n",
    "    obs2 = torch.tensor(obs_list[1], dtype=torch.float).permute(2, 0, 1)\n",
    "    \n",
    "    last_100_rew = [0 for i in range(100)]\n",
    "    for i in range(max_iter):\n",
    "        act1 = agent1.choose_act(obs1)\n",
    "        act2 = agent2.choose_act(obs2)\n",
    "        act_list = [act1, act2]\n",
    "        # print(\"iter= \", i, env.agt1_pos, env.agt2_pos, env.pig_pos, env.agt1_ori, env.agt2_ori, 'action', act1, act2)\n",
    "        env.render()\n",
    "        rew_list, done = env.step(act_list)\n",
    "        rew1 = rew_list[0]\n",
    "        rew2 = rew_list[1]\n",
    "        # print(rew1)\n",
    "        _obs_list = env.get_obs()\n",
    "        _obs1 = torch.tensor(_obs_list[0], dtype=torch.float).permute(2, 0, 1)\n",
    "        _obs2 = torch.tensor(_obs_list[1], dtype=torch.float).permute(2, 0, 1)\n",
    "        agent1.store_transition(obs1, act1, rew1, _obs1, done)\n",
    "        agent2.store_transition(obs2, act2, rew2, _obs2, done)\n",
    "        obs1 = _obs1\n",
    "        obs2 = _obs2\n",
    "        agent1.learn()\n",
    "        agent2.learn()\n",
    "        last_100_rew[i % 100] = rew1 + rew2\n",
    "        \n",
    "        #env.plot_scene()\n",
    "        if rew1 + rew2 > 0:\n",
    "            print(\"iter= \", i)\n",
    "            print(\"Goal found!\")\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iter: {i}, Epsilon:{agent1.epsilon}, Reward: {sum(last_100_rew)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "convenient-inquiry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of map should be an odd integer no smaller than 7\n",
      "Iter: 0, Epsilon:1, Reward: 0\n",
      "Iter: 100, Epsilon:0.9790000000000023, Reward: -580\n",
      "Iter: 200, Epsilon:0.9490000000000056, Reward: -600\n",
      "Iter: 300, Epsilon:0.9190000000000089, Reward: -580\n",
      "Iter: 400, Epsilon:0.8890000000000122, Reward: -660\n",
      "Iter: 500, Epsilon:0.8590000000000155, Reward: -400\n",
      "Iter: 600, Epsilon:0.8290000000000188, Reward: -380\n",
      "Iter: 700, Epsilon:0.7990000000000221, Reward: -460\n",
      "Iter: 800, Epsilon:0.7690000000000254, Reward: -520\n",
      "Iter: 900, Epsilon:0.7390000000000287, Reward: -360\n",
      "Iter: 1000, Epsilon:0.709000000000032, Reward: -460\n",
      "Iter: 1100, Epsilon:0.6790000000000354, Reward: -200\n",
      "Iter: 1200, Epsilon:0.6490000000000387, Reward: -420\n",
      "Iter: 1300, Epsilon:0.619000000000042, Reward: -220\n",
      "Iter: 1400, Epsilon:0.5890000000000453, Reward: -160\n",
      "Iter: 1500, Epsilon:0.5590000000000486, Reward: -240\n",
      "Iter: 1600, Epsilon:0.5290000000000519, Reward: -400\n",
      "Iter: 1700, Epsilon:0.49900000000005496, Reward: -220\n",
      "Iter: 1800, Epsilon:0.4690000000000527, Reward: -200\n",
      "Iter: 1900, Epsilon:0.43900000000005046, Reward: -180\n",
      "Iter: 2000, Epsilon:0.4090000000000482, Reward: -220\n",
      "Iter: 2100, Epsilon:0.37900000000004597, Reward: -220\n",
      "Iter: 2200, Epsilon:0.3490000000000437, Reward: -320\n",
      "Iter: 2300, Epsilon:0.3190000000000415, Reward: -200\n",
      "Iter: 2400, Epsilon:0.2890000000000392, Reward: -260\n",
      "Iter: 2500, Epsilon:0.259000000000037, Reward: -120\n",
      "Iter: 2600, Epsilon:0.22900000000003667, Reward: -100\n",
      "Iter: 2700, Epsilon:0.1990000000000372, Reward: -100\n",
      "Iter: 2800, Epsilon:0.16900000000003773, Reward: -40\n",
      "Iter: 2900, Epsilon:0.13900000000003826, Reward: -40\n",
      "Iter: 3000, Epsilon:0.10900000000003879, Reward: -100\n",
      "Iter: 3100, Epsilon:0.07900000000003932, Reward: -60\n",
      "Iter: 3200, Epsilon:0.04900000000003953, Reward: -60\n",
      "iter=  3214\n",
      "Goal found!\n",
      "Iter: 3300, Epsilon:0.019000000000039367, Reward: 980\n",
      "Iter: 3400, Epsilon:0.01, Reward: 0\n",
      "Iter: 3500, Epsilon:0.01, Reward: 0\n",
      "Iter: 3600, Epsilon:0.01, Reward: 0\n",
      "Iter: 3700, Epsilon:0.01, Reward: 0\n",
      "Iter: 3800, Epsilon:0.01, Reward: -20\n",
      "Iter: 3900, Epsilon:0.01, Reward: 0\n",
      "Iter: 4000, Epsilon:0.01, Reward: -20\n",
      "Iter: 4100, Epsilon:0.01, Reward: 0\n",
      "Iter: 4200, Epsilon:0.01, Reward: 0\n",
      "Iter: 4300, Epsilon:0.01, Reward: -20\n",
      "Iter: 4400, Epsilon:0.01, Reward: -40\n",
      "Iter: 4500, Epsilon:0.01, Reward: -20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-101ebb6961f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdqn2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdqn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdqn2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-37c1013d4545>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(max_iter, env, agent1, agent2)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mobs2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_obs2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0magent1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0magent2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mlast_100_rew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrew1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrew2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-bdc27ae3915f>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_cnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/rl/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/rl/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = EnvCatchPigs(7, False)\n",
    "max_iter = 5000\n",
    "\n",
    "dqn1 = DQNAgent((3,21,21), 5)\n",
    "dqn2 = DQNAgent((3,21,21), 5)\n",
    "\n",
    "train(max_iter, env, dqn1, dqn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-dollar",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn1.epsilon = 0.6\n",
    "dqn2.epsilon = 0.6\n",
    "train(10000, env, dqn1, dqn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-riverside",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

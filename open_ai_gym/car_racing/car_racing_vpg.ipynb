{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPGBuffer():\n",
    "    def __init__(self, size, obs_dim, act_dim, gamma=0.99, lam=0.95):\n",
    "        self.obs_buf = np.zeros((size, *obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros((size, act_dim), dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.max_size = size\n",
    "        self.ptr = 0\n",
    "        self.traj_start_idx = 0\n",
    "\n",
    "\n",
    "    def _discount_cumsum(self, x, discount):\n",
    "        \"\"\"\n",
    "        The code below calculates the cummulative discounted sum.\n",
    "        A more efficient way of doing it, but less readible is the following:\n",
    "            return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "        \"\"\"\n",
    "        cumsum = np.zeros(len(x), dtype=np.float32)\n",
    "        cumsum[-1] = x[-1]\n",
    "        for i in range(len(x) - 2, -1, -1):\n",
    "            cumsum[i] = x[i] + discount * cumsum[i+1]\n",
    "        return cumsum\n",
    "        \n",
    "\n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        assert self.ptr < self.max_size # there must be space in the buffer to store\n",
    "\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "\n",
    "    def end_trajectory(self, last_val):\n",
    "        traj_slice = slice(self.traj_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[traj_slice], last_val)\n",
    "        vals = np.append(self.val_buf[traj_slice], last_val)\n",
    "\n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[traj_slice] = self._discount_cumsum(deltas, self.gamma * self.lam)\n",
    "\n",
    "        # the next line computes the reward to go\n",
    "        self.ret_buf[traj_slice] = self._discount_cumsum(rews, self.gamma)[:-1]\n",
    "\n",
    "        self.traj_start_idx = self.ptr\n",
    "\n",
    "\n",
    "    def get(self):\n",
    "        assert self.ptr == self.max_size # buffer must be full\n",
    "\n",
    "        # reset the buffer\n",
    "        self.ptr, self.traj_start_idx = 0, 0\n",
    "\n",
    "        # normalize advantages for training stability\n",
    "        adv_mean = np.mean(self.adv_buf)\n",
    "        adv_std = np.std(self.adv_buf)\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
    "\n",
    "        data = dict(obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf, adv=self.adv_buf, logp=self.logp_buf)\n",
    "\n",
    "        # convert data to dict of torch tensors\n",
    "        data = {k: torch.as_tensor(v, dtype=torch.float32) for k,v in data.items()}\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        modules = []\n",
    "        modules.append(nn.Conv2d(obs_dim[0], 32, 8, stride=4))\n",
    "        modules.append(nn.ReLU())\n",
    "        modules.append(nn.Conv2d(32, 64, 4, stride=2))\n",
    "        modules.append(nn.ReLU())\n",
    "        modules.append(nn.Conv2d(64, 64, 3, stride=1))\n",
    "        modules.append(nn.ReLU())\n",
    "\n",
    "        def _calc_conv_output_dims(input_dim):\n",
    "            tmp = torch.zeros((1, *input_dim))\n",
    "            tmp = modules[0](tmp)\n",
    "            tmp = modules[2](tmp)\n",
    "            tmp = modules[4](tmp)\n",
    "            return int(np.prod(tmp.size()))\n",
    "\n",
    "        linear_input_dims = _calc_conv_output_dims(obs_dim)\n",
    "\n",
    "        class Flatten(nn.Module):\n",
    "            def __init__(self, linear_input_dims):\n",
    "                super().__init__()\n",
    "                self.linear_input_dims = linear_input_dims\n",
    "\n",
    "            def forward(self, obs):\n",
    "                return obs.view(-1, linear_input_dims)\n",
    "        \n",
    "        modules.append(Flatten(linear_input_dims))\n",
    "        modules.append(nn.Linear(linear_input_dims, 512))\n",
    "        modules.append(nn.Linear(512, act_dim))\n",
    "\n",
    "        log_std = -0.5 * np.ones(act_dim, dtype=np.float32)\n",
    "        self.log_std = torch.nn.Parameter(torch.as_tensor(log_std))\n",
    "        self.mu_net = nn.Sequential(*modules)\n",
    "\n",
    "\n",
    "    def _distribution(self, obs):\n",
    "        mu = self.mu_net(obs)\n",
    "        std = torch.exp(self.log_std)\n",
    "        return Normal(mu, std)\n",
    "\n",
    "    \n",
    "    def _log_prob_from_distribution(self, pi, act):\n",
    "        return pi.log_prob(act).sum(axis=-1)    # Last axis sum needed for Torch Normal distribution\n",
    "\n",
    "    \n",
    "    def forward(self, obs, act=None):\n",
    "        # Produce action distributions for given observations, and \n",
    "        # optionally compute the log likelihood of given actions under\n",
    "        # those distributions.\n",
    "        pi = self._distribution(obs)\n",
    "        logp_a = None\n",
    "        if act is not None:\n",
    "            logp_a = self._log_prob_from_distribution(pi, act)\n",
    "        return pi, logp_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim):\n",
    "        super().__init__()\n",
    "        modules = []\n",
    "        modules.append(nn.Conv2d(obs_dim[0], 32, 8, stride=4))\n",
    "        modules.append(nn.ReLU())\n",
    "        modules.append(nn.Conv2d(32, 64, 4, stride=2))\n",
    "        modules.append(nn.ReLU())\n",
    "        modules.append(nn.Conv2d(64, 64, 3, stride=1))\n",
    "        modules.append(nn.ReLU())\n",
    "\n",
    "        def _calc_conv_output_dims(input_dim):\n",
    "            tmp = torch.zeros((1, *input_dim))\n",
    "            tmp = modules[0](tmp)\n",
    "            tmp = modules[2](tmp)\n",
    "            tmp = modules[4](tmp)\n",
    "            return int(np.prod(tmp.size()))\n",
    "\n",
    "        linear_input_dims = _calc_conv_output_dims(obs_dim)\n",
    "\n",
    "        class Flatten(nn.Module):\n",
    "            def __init__(self, linear_input_dims):\n",
    "                super().__init__()\n",
    "                self.linear_input_dims = linear_input_dims\n",
    "\n",
    "            def forward(self, obs):\n",
    "                return obs.view(-1, linear_input_dims)\n",
    "        \n",
    "        modules.append(Flatten(linear_input_dims))\n",
    "        modules.append(nn.Linear(linear_input_dims, 512))\n",
    "        modules.append(nn.Linear(512, 1))\n",
    "        self.v_net = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return torch.squeeze(self.v_net(obs), -1) # Critical to ensure v has right shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.v = Critic(obs_dim)\n",
    "        self.pi = Actor(obs_dim, act_dim)\n",
    "\n",
    "    def step(self, obs):\n",
    "        with torch.no_grad():\n",
    "            pi = self.pi._distribution(obs)\n",
    "            a = pi.sample()\n",
    "            logp_a = self.pi._log_prob_from_distribution(pi, a)\n",
    "            v = self.v(obs)\n",
    "        return a.numpy(), v.numpy(), logp_a.numpy()\n",
    "\n",
    "    def act(self, obs):\n",
    "        return self.step(obs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPGAgent():\n",
    "    def __init__(self, obs_dim, act_dim, gamma=0.99, pi_lr=3e-4, vf_lr=1e-3, lam=0.97, train_v_iters=10, steps_per_epoch=4000):\n",
    "        \n",
    "        self.buf = VPGBuffer(steps_per_epoch, obs_dim, act_dim, gamma, lam)\n",
    "\n",
    "        self.ac = ActorCritic(obs_dim, act_dim)\n",
    "\n",
    "        # Set up optimizers for policy and value function\n",
    "        self.pi_optimizer = optim.Adam(self.ac.pi.parameters(), lr=pi_lr)\n",
    "        self.vf_optimizer = optim.Adam(self.ac.v.parameters(), lr=vf_lr)\n",
    "\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.train_v_iters = train_v_iters\n",
    "\n",
    "\n",
    "    def _compute_loss_pi(self, data):\n",
    "        obs, act, adv, logp_old = data['obs'], data['act'], data['adv'], data['logp']\n",
    "\n",
    "        # Policy loss\n",
    "        pi, logp = self.ac.pi(obs, act)\n",
    "        loss_pi = -(logp * adv).mean() # negative log probability loss\n",
    "\n",
    "        return loss_pi\n",
    "\n",
    "\n",
    "    def _compute_loss_v(self, data):\n",
    "        obs, ret = data['obs'], data['ret']\n",
    "        return ((self.ac.v(obs) - ret)**2).mean() # MSE  \n",
    "\n",
    "    def update(self):\n",
    "        data = self.buf.get()\n",
    "\n",
    "        # Train policy with a single step of gradient descent\n",
    "        self.pi_optimizer.zero_grad()\n",
    "        loss_pi = self._compute_loss_pi(data)\n",
    "        loss_pi.backward()\n",
    "        self.pi_optimizer.step()\n",
    "\n",
    "        for i in range(self.train_v_iters):\n",
    "            self.vf_optimizer.zero_grad()\n",
    "            loss_v = self._compute_loss_v(data)\n",
    "            loss_v.backward()\n",
    "            self.vf_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env, epochs, max_ep_len=1000):\n",
    "    o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "    o = o.transpose(2,0,1)\n",
    "    ep_rets = []\n",
    "    for epoch in range(epochs):\n",
    "        for t in range(agent.steps_per_epoch):\n",
    "            a, v, logp = agent.ac.step(torch.as_tensor(np.ascontiguousarray(o), dtype=torch.float32).unsqueeze(0))\n",
    "            a = a.squeeze(0)\n",
    "            next_o, r, d, _ = env.step(a)\n",
    "            next_o = next_o.transpose(2,0,1)\n",
    "            ep_ret += r\n",
    "            ep_len += 1\n",
    "\n",
    "            agent.buf.store(o, a, r, v, logp)\n",
    "\n",
    "            o = next_o\n",
    "\n",
    "            timeout = ep_len == max_ep_len\n",
    "            terminal = d or timeout\n",
    "            epoch_ended = t == agent.steps_per_epoch - 1\n",
    "\n",
    "            if terminal or epoch_ended:\n",
    "                if timeout or epoch_ended:\n",
    "                    _, v, _ = agent.ac.step(torch.as_tensor(np.ascontiguousarray(o), dtype=torch.float32).unsqueeze(0))\n",
    "                else:\n",
    "                    v = 0\n",
    "                agent.buf.end_trajectory(v)\n",
    "                ep_rets.append(ep_ret)\n",
    "                o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "                o = o.transpose(2,0,1)\n",
    "\n",
    "        agent.update()\n",
    "\n",
    "        print('Epoch: ', epoch,'avg ep_ret: ', np.mean(ep_rets[-10:]), \"total num ep: \", len(ep_rets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v0')\n",
    "agent = VPGAgent((3, 96, 96), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Track generation: 1112..1394 -> 282-tiles track\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-4f92f5ecf487>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-4ab6e3a53924>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(agent, env, epochs, max_ep_len)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mnext_o\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mnext_o\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mep_ret\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/rl/lib/python3.8/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/rl/lib/python3.8/site-packages/gym/envs/box2d/car_racing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mFPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mFPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mFPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/rl/lib/python3.8/site-packages/gym/envs/box2d/car_dynamics.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, dt)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# Steer each wheel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteer\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mangle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteer\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mangle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotorSpeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50.0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(agent, env, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}